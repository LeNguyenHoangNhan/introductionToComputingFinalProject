<!DOCTYPE html>

<html>

<head>
    <title>Deep learning in a nutshell</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta charset="UTF-8" />
    <link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
    <link rel="icon" type="image/png" sizes="192x192" href="android-icon-192x192.png">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="manifest" href="manifest.json">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="msapplication-TileImage" content="ms-icon-144x144.png">
    <meta name="theme-color" content="#ffffff">
    <link rel="shortcut icon" href="favicon.ico" />
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <div class="page-wrapper">
        <article id="main">
            <div class="article-cover">
                <img src="deeplearning.jpg" class="cover-img" />
            </div>
            <h1 class="title">Deep learning in a nutshell</h1>
            <p class="time-location"><i>Ho Chi Minh City - Jan 01, 2021</i></p>
            <section class="intro">
                <h2>Introduction</h2>
                <p class="content">
                    Deep Learning, a sub-area of Machine Learning has
                    been applied in many tasks such as Computer Vision,
                    Natural Language Processing, Speech Recognition, etc
                    with noticeably high accuracy. Deep Learning has experienced a significant evolution and has become
                    a stateof-the-art technique in processing a huge amount of
                    data and imitating the thinking process. In this report, we want to represent a thorough view of
                    this term.
                    Technically, the field of Artificial Intelligence is focusing
                    on
                </p>
            </section>
            <section class="content">
                <h2>What is deep learning</h2>
                <p class="content">
                    Let’s start with most common term that you have heard
                    so far - Artificial Intelligence (AI). Artificial Intelligence
                    can be simply defined as an ability to process information to inform future decisions. Machine
                    Learning, in
                    general, concentrates on how to create algorithms that
                    let machines have ability to learn from historical data
                    without being explicitly programmed.
                </p>
                <div class="content-img-wrapper">
                    <img src="layer.png" class="content-img" />
                    <p class="content-img-caption">
                        <i>General view of AI, Machine Learning, and Deep Learning</i>
                    </p>
                </div>

                <p class="content">
                    Finally, Deep Learning is a branch of Machine Learning,
                    using the Artificial Neural Networks (ANN) to adapt and learn from a vast of data. The main
                    difference between Deep Learning and Machine Learning is the
                    way that data is structured. Machine Learning algorithms
                    almost require labeled data and they are designed to
                    understand the labeled data in order to predict future
                    result. Whereas, Deep Learning do not required structured data (or human intervention) because they
                    almost rely on layers of ANNs.
                </p>
                <div class="content-img-wrapper">
                    <img src="network.png" class="content-img" />
                    <p class="content-img-caption">
                        <i>Artificial Network Architecture</i>
                    </p>
                </div>
                <p class="content">
                <blockquote class="quotes">
                    <i>
                        "When you hear the term ‘Deep Learning’, just
                        think of a large Deep Neural Network. ‘Deep’
                        refers to the number of layers typically and so
                        this is kind of the popular term that’s have been
                        adopted in the press. I think of them as Deep Neural Networks generally."
                    </i>
                    <p class="quote-author">−Jeff Dean, Google Senior Fellow in the System and Infrastructure Group</p>
                </blockquote>
                </p>
            </section>
            <section class="content">
                <h2>AI winter to an evolution</h2>
                <p class="content">
                    The history of Deep Learning can be traced back
                    to 1943, when Walter Pitts and Warren McCulloch
                    created a computer model based on the neural networks
                    of the human brain. A combination of algorithms and
                    mathematics, called ‘threshold logic’, were used to
                    mimic the thought process. From that time forward,
                    Deep Learning has evolved steadily, with only two
                    significant break-through in its development. Both
                    were tied to the infamous Artificial Intelligence winters.
                </p>
                <p class="content">The earliest efforts in developing Deep Learning algorithms dated to 1965, when
                    Alexey Grigoryevich
                    Ivakhnenko used models with polynomial (complication equations) activation functions, which were
                    subsequently analysed statistically. During the 1970’s
                    a brief setback was felt into the development of AI,
                    lack of funding limited both Deep Learning and
                    Artificial Intelligence research, but individuals carried
                    on researches without funding through those difficult
                    years.
                </p>
                <p class="content">
                    Convolutional Neural Networks (CNNs) were first
                    used by Kunihiko Fukushima who designed the neural
                    networks with multiple pooling and convolutional
                    layers. He developed an ANN, called Neocognitron
                    in 1979, which used a multi-layered and hierarchical
                    design
                </p>
                <p class="content">
                    In 1970’s, Backpropagation, was developed which
                    uses errors into training Deep Learning models. Backpropagation became popular when Seppo Linnainmaa
                    wrote his master’s thesis, including a FORTRAN
                    code for backpropagation. Though developed in the
                    1970’s, the concept was not applied to neural networks
                    until 1985 when Hinton and Rumelhart, Williams
                    demonstrated backpropagation in a neural network
                    which could provide interesting distribution representations. Yann LeCun explained the first
                    practical
                    demonstration of backpropagation at Bell Labs in 1989
                    by combining CNNs with back propagation to read
                    handwritten digits. The combination of CNNs with
                    backpropagation system was used to read the numbers
                    of handwritten checks.
                </p>
                <p class="content">
                    1985−90s kicked the second lull into Artificial Intelligence which effected research for neural
                    networks
                    and Deep Learning. Going on over the years, in 1995
                    Vladimir Vapnik and Dana Cortes developed the
                    Support Vector Machine (SVM) which is a system
                    for mapping and recognizing similar data. Long
                    Short-Term Memory (LSTM) was developed in 1997
                    by Juergen Schmidhuber and Sepp Hochreiter for
                    Recurrent Neural Networks (RNNs).
                </p>
                <p class="content">
                    The next significant Deep Learning advancement
                    was in 1999 when computers adopted the speed of the
                    GPU processing. Faster processing meant increased
                    computational speeds of 1000 times over a 10-year
                    span. This era meant neural networks began competing
                    with SVMs Neural networks offered better results using
                    the same data, though slow to a SVM.
                </p>
                <p class="content">
                    The Vanishing Gradient Problem came out in the
                    year 2000 when “features” (lessons) formed in lower
                    layers were not being learned by the upper layers since
                    no learning signal reached these layers were discovered.
                    This was not a fundamental problem for all neural
                    networks but is restricted to only Gradient-Based
                    Learning methods. This problem turned out to be
                    certain activation functions which condensed their
                    input and reduced the output range in a chaotic fashion. This led to large areas of input mapped
                    over
                    an extremely small range.
                </p>
                <p class="content">
                    Fei-Fei Li, an AI professor at Stanford launched
                    ImageNet in 2009 assembling a free database of more
                    than 14 million labeled images. These images were the
                    inputs to train neural nets. The speed of GPUs had
                    increased significantly by 2011, making it possible to
                    train CNNs without the need of layer by layer pretraining. Deep learning holds significant
                    advantages
                    into efficiency and speed.
                </p>
                <p class="content">
                    In 2012, Google Brain released the results of an
                    unusual free-spirited project called the Cat Experiment
                    which explored the difficulties of Unsupervised Learning. Deep learning deploys Supervised Learning,
                    which
                    means the CNNs are trained using labeled data like the
                    images from ImageNet. This experiment used a neural
                    net which was spread over 1,000 computers where ten
                    million unlabelled images were taken randomly from
                    YouTube, as inputs to the training software. From
                    that year onwards, unsupervised learning remains a
                    significant goal in the field of Deep Learning.
                </p>
                <p class="content">
                <blockquote class="quotes">
                    <i>
                        "I think people need to understand that Deep
                        Learning is making a lot of things, behind-thescenes, much beter."
                    </i>
                    <p class="quote-author">−Jeff Dean, Google Senior Fellow in the System and Infrastructure Group</p>
                </blockquote>
                </p>
                <section class="sub">
                    <h3>A Timeline of deep learning development</h3>
                    <ul class="list">
                        <li class="list-item"></li>
                        <strong>1960s:</strong> Shallow Neural Networks.
                        </li>
                        <li class="list-item"></li>
                        <strong>1960−70s:</strong> Backpropagation emerges.
                        </li>
                        <li class="list-item"></li>
                        <strong>1974−80s:</strong> First AI Winter.
                        </li>
                        <li class="list-item"></li>
                        <strong>1980s:</strong> Convolution emerges
                        </li>
                        <li class="list-item"></li>
                        <strong>1987−93:</strong> Second AI Winter.
                        </li>
                        <li class="list-item"></li>
                        <strong>1990s:</strong> Unsupervised Deep Learning.
                        </li>
                        <li class="list-item"></li>
                        <strong>1990s−2000s:</strong> Supervised Deep Learning
                        </li>
                        <li class="list-item"></li>
                        <strong>2000s−:</strong> Modern Deep Learning
                        </li>
                    </ul>
                </section>
            </section>
            <section class="content">
                <h2>Deep learning architectures</h2>
                <section class="sub">
                    <h3 class="sub-title">Convulational neural networks (CNNs)</h3>
                    <p class="content">
                        Convolutional Neural Networks (CNNs) are one
                        of the most popular architectures desgined for various
                        Computer Vision tasks such as Image Classification,
                        Image Recognition, etc. by Kunihiko Fukushima.
                    </p>
                    <p class="content">
                        A Convolutional Neural Network (ConvNet/CNN)
                        is a Deep Learning algorithm which can take in an
                        input image, assign importance (learnable weights
                        and biases) to various aspects/objects in the image and be able to differentiate one from the
                        other. The pre-processing required in a ConvNet
                        is much lower as compared to other classification
                        algorithms. While in primitive methods filters are
                        hand-engineered, with enough training, ConvNets
                        have the ability to learn these filters/characteristics.
                    </p>
                    <div class="content-img-wrapper">
                        <img src="cnns.jpg" class="content-img" />
                        <p class="content-img-caption">
                            <i>Convolutional Neural Networks for Image Recognition</i>
                        </p>
                    </div>
                    <p class="content">
                        The architecture of a ConvNet is analogous to
                        that of the connectivity pattern of Neurons in the
                        Human Brain and was inspired by the organization
                        of the Visual Cortex. Individual neurons respond to
                        stimuli only in a restricted region of the visual field
                        known as the Receptive Field. A collection of such
                        fields overlap to cover the entire visual area.
                    </p>
                </section>
                <section class="sub">
                    <h3 class="sub-title">Recurrent neural networks (RNNs)</h3>
                    <p class="content">
                        A Recurrent Neural Network (RNNs) is a type of
                        ANNs which uses sequential data or time series data.
                        These Deep Learning algorithms are commonly used
                        for ordinal or temporal problems, such as Language
                        Translation, Natural Language Processing (NLP),
                        Speech Recognition, and Image Captioning; they are
                        incorporated into popular applications such as Siri,
                        voice search, and Google Translate. Like Feedforward
                        and Convolutional Neural Networks (CNNs), Recurrent
                        Neural Networks utilize training data to learn. They
                        are distinguished by their “memory” as they take information from prior inputs to influence the
                        current input
                        and output. While traditional Deep Neural Networks
                        assume that inputs and outputs are independent of
                        each other, the output of recurrent neural networks depend on the prior elements within the
                        sequence. While
                        future events would also be helpful in determining the
                        output of a given sequence, unidirectional Recurrent
                        Neural Networks cannot account for these events in
                        their predictions. Another distinguishing characteristic
                        of Recurrent Neural Networks is that they share
                        parameters across each layer of the network. While
                        Feedforward Networks have different weights across
                        each node, Recurrent Neural Networks share the same
                        weight parameter within each layer of the network.
                    </p>
                    <div class="content-img-wrapper">
                        <img src="rnn.png" class="content-img" />
                        <p class="content-img-caption">
                            <i>Types of Recurrent Neural Networks</i>
                        </p>
                    </div>
                    <div class="content">
                        <h5 class="subsub-title">Types of recurrent neural networks</h5>
                        <ul class="list">
                            <li class="list-item"></li>
                            One−to−One
                            </li>
                            <li class="list-item"></li>
                            One−to−Many
                            </li>
                            <li class="list-item"></li>
                            Many−to−One
                            </li>
                            <li class="list-item"></li>
                            Many−to−Many
                            </li>

                        </ul>
                    </div>

                </section>
                <section class="sub">
                    <h3 class="sub-title">Long-short term memory (LSTM)</h3>
                    <p class="content">
                        Long-Short Term Memory Networks (LSTMs) – are a
                        special kind of RNNs, capable of learning long-term
                        dependencies. They were introduced by Hochreiter
                        Schmidhuber (1997), and were refined and popularized by many people. They work tremendously
                        well on a large variety of problems, and are now
                        widely used. LSTMs are explicitly designed to avoid
                        the long-term dependency problem. Remembering
                        information for long periods of time is practically
                        their default behavior, not something they struggle
                        to learn. All Recurrent Neural Networks have the
                        form of a chain of repeating modules of neural network. In standard RNNs, this repeating module
                        will
                        have a very simple structure, such as a single tanh layer.
                    </p>
                    <div class="content-img-wrapper">
                        <img src="lstm.png" class="content-img" />
                        <p class="content-img-caption">
                            <i> A basic structure of Long−Short Term
                                Memory</i>
                        </p>
                    </div>
                    <div class="content">
                        <h5 class="subsub-title">Applications of LSTMs:</h5>
                        <ul class="list">
                            <li class="list-item">
                                Speech Recognition − performed by Google Assistant, Microsoft Cortana, Apple Siri.
                            </li>
                            <li class="list-item">
                                • Machine Translation − performed by Google
                                Translate.
                            </li>
                            <li class="list-item">
                                Music Generation − done by Bao Dai, Research
                                Scientist at Knorex, Teaching VietAI.
                            </li>
                        </ul>
                    </div>
                </section>

                <section class="sub">
                    <h3 class="sub-title">Autoencoder</h3>
                    <p class="content">
                        Autoencoders are an unsupervised learning technique in
                        which we leverage neural networks for the task of representation learning. Specifically, we’ll
                        design a neural
                        network architecture such that we impose a bottleneck
                        in the network which forces a compressed knowledge
                        representation of the original input. If the input features were each independent of one
                        another, this compression and subsequent reconstruction would be a very
                        difficult task. However, if some sort of structure exists
                        in the data (ie. correlations between input features), this structure can be learned and
                        consequently leveraged when forcing the input through the network’s bottleneck.
                    </p>
                    <div class="content-img-wrapper">
                        <img src="autoencoder.png" class="content-img" />
                        <p class="content-img-caption">
                            <i>Autoencoder for reconstructing image</i>
                        </p>
                    </div>
                    <p class="content">
                        As we can observe, we can take an unlabeled dataset
                        and frame it as a Supervised Learning problem tasked
                        with outputs, a reconstruction of inputs. This network
                        can be trained by minimizing the reconstruction error,
                        which measures the difference between our initial input
                        and the final result. The bottleneck is a key attribute
                        of our network desgin; with the presence of bottleneck,
                        our network could easily learn to simply memorize the
                        input values by passing these values along through the
                        network.
                    </p>
                    <div class="content-img-wrapper">
                        <img src="autoencoder2.png" class="content-img" />
                        <p class="content-img-caption">
                            <i>A construction of Autoencoder model</i>
                        </p>
                    </div>
                    <div class="content">
                        <h5 class="subsub-title">Applications of Autoencoder:</h5>
                        <ul class="list">
                            <li class="list-item">
                                Anomaly Detection.
                            </li>
                            <li class="list-item">
                                Data Denoising (e.g. images, audio, etc.).
                            </li>
                            <li class="list-item">
                                Image Inpainting. Retrieval.
                            </li>
                        </ul>
                    </div>
                </section>
                <section class="sub">
                    <h3 class="sub-title">Generative adversarial network (GAN)</h3>
                    <p class="content">
                        Generative Adversarial Networks, (GANs), are an approach to generative modeling using Deep
                        Learning
                        methods, such as Convolutional Neural Networks.
                    </p>
                    <div class="content-img-wrapper">
                        <img src="gan1.png" class="content-img" />
                        <p class="content-img-caption">
                            <i>The Generative Adversarial Networks</i>
                        </p>
                    </div>
                    <p class="content">
                        Generative modeling is an Unsupervised Learning
                        task in Machine Learning that involves automatically
                        discovering and learning the regularities or patterns in
                        input data in such a way that the model can be used to
                        generate or output new examples that plausibly could
                        have been drawn from the original dataset. It means
                        that they are able to create (or generate) some new
                        contents. To illustrate the term ‘Generative models’,
                        we can consider some well-known examples obtained
                        from GANs.
                    </p>
                    <div class="content-img-wrapper">
                        <img src="gan2.png" class="content-img" />
                        <p class="content-img-caption">
                            <i>Generative Adversarial Networks for Human Faces.</i>
                        </p>
                    </div>
                    <p class="content">
                        Generative Adversarial Networks are an exciting and
                        rapidly transforming field, delivering on the promise of
                        generative models in their ability to generate realistic
                        examples across a range of problems domain, most
                        notably in image2image translation tasks, such as
                        translating photos of horses to zebras, or replace
                        another tone for a picture, etc.
                    </p>
                    <div class="content">
                        <h5 class="subsub-title">Application of generative adversarial networks:</h5>
                        <ul class="list">
                            <li class="list-item">
                                Generating examples for Image Dataset.
                            </li>
                            <li class="list-item">
                                Generating photographs of human faces.
                            </li>
                            <li class="list-item">
                                Generating realistic Photographs.
                            </li>
                            <li class="list-item">
                                Image2Image translation.
                            </li>
                            <li class="list-item">
                                Text2Image translation, etc.
                            </li>
                        </ul>
                    </div>
                </section>
            </section>
            <section class="content">
                <h2>Achievement</h2>
                <p class="content">
                    In general, Artificial Intelligence (AI), Machine Learning (ML), and Deep Learnign (DL) have turned
                    out to
                    be the life-changing technologies over the last few years.
                    AI provides extraodinary solutions for many fields and
                    every companies, it has also changed the lanscape of
                    health, living, learning and other daily dealings. Companies and researchers have made significant
                    progress
                    and accomplish many achievements in automic text
                    generation, translation, facial and sound recognition,
                    detect motion, drug discovery, etc. Let’s go through
                    some break-through achievements in Deep Learning.
                </p>
                <section class="sub">
                    <h3 class="sub-title">Computer Vision</h3>
                    <h4>Video analysis & Image processing</h4>
                    <p class="content">
                        The widespread proliferation of digital media, and in
                        particular video data, in recent years, has made the automated analysis for retrieval, storage,
                        transmission, secuirty, and commercial purposes. Applications include
                        health monitoring for ambient assisted living, analysis
                        of online content, effective classification, or in security
                        and surveillance applications, in the case of traffic or
                        crowded. Advancement in Artificial Intelligence and
                        Deep Learning has made video or image analysis more
                        feasible with cheaper investment. Computer Vision are
                        greatly solving the problems related to social issuses or
                        manufacturing industry in offering automatic quality
                        control and safety mangament techniques.
                    </p>
                    <div class="content-img-wrapper">
                        <img src="cv.jpg" class="content-img" />
                        <p class="content-img-caption">
                            <i> Computer vision technique in analysing
                                activities of objects in video.</i>
                        </p>
                    </div>
                    <h4>Face Recognition</h4>
                    <p class="content">
                        Image processing and patterns recognition are one of
                        the most significant achievements of Deep Learning
                        in 2020. Nowadays, most of smartphone users are
                        familiar with Facial Identity Recognition. Significantly high accuracy in recognizing face
                        identity
                        has been accomplished, it can work well even with
                        the presence of other objects such as mask. This
                        technology was introduced by VinAI Research Lab,
                        and ranked 6th out of 430 solutions for Face Recognition Vendor Test (FRVT) by National
                        Institute of
                        Standards and Technology, US (NIST). All Vsmart
                        smartphones are equipped this new technology, enable users to unlock their phone via their face,
                        and
                        hopefully it can be commmercialized in nearly future.
                    </p>
                    <div class="content-img-wrapper">
                        <img src="vinface.png" class="content-img" />
                        <p class="content-img-caption">
                            <i>A new technology of VinAI in recognizing
                                human face with mask.</i>
                        </p>
                    </div>
                </section>
                <section class="sub">
                    <h3 class="sub-title">Natural Language Processing</h3>
                    <p class="content">
                        The widespread proliferation of digital media, and in
                        particular video data, in recent years, has made the automated analysis for retrieval, storage,
                        transmission, secuirty, and commercial purposes. Applications include
                        health monitoring for ambient assisted living, analysis
                        of online content, effective classification, or in security
                        and surveillance applications, in the case of traffic or
                        crowded. Advancement in Artificial Intelligence and
                        Deep Learning has made video or image analysis more
                        feasible with cheaper investment. Computer Vision are
                        greatly solving the problems related to social issuses or
                        manufacturing industry in offering automatic quality
                        control and safety mangament techniques.
                    </p>
                    <div class="content">
                        <ul class="list">
                            <li class="list-item">
                                <strong>Text Labeling:</strong> Companies like Google filter and
                                classify your emails with NLP by analysing text
                                in emails that flow through their servers and label
                                whether an email is spam or not. Then they will
                                stop spam before an email even enter your inbox.
                            </li>
                            <li class="list-item">
                                <strong>Sentiment Analysis:</strong> Organizations can determine customer’s atitude via
                                what they are saying
                                about the services or product by using NLP. This
                                sentiment analysis can provide lots of information
                                about the behaviour of customers so that companies can adapt and come to right
                                strategies in their
                                business.
                            </li>
                            <li class="list-item">
                                <strong>Question and Answering:</strong> Chatbot and Virtual
                                Assistants are on the go in these cases. Amazon’s
                                Alexa and Apple’s Siri are typical examples for this
                                application of NLP. Thanks to some state-of-theart pre-trained Natural Language
                                Processing (such
                                as ULMFiT, Bert, etc.) models for chatbots that
                                help them can perform much more accurately and
                                give users better experience.
                        </ul>
                    </div>
                    <div class="content-img-wrapper">
                        <img src="google.png" class="content-img" />
                        <p class="content-img-caption">
                            <i> Virtual Assistant - Alexa, Amazon</i>
                        </p>
                    </div>
                    <h4>The first Pre-trained Natural Langugae Processing model for Vietnamese</h4>
                    <p class="content">
                        PhoBert [] is the first public large-scale language models
                        for Vietnamese created by Dat Quoc Nguyen - Senior
                        Research Scientist, VinAI Research, Vietnam and
                        Tuan Anh Nguyen − Research Intern, NVIDIA, USA.
                        They present the first large-scale monolingual language
                        models for Vietnamese, which help produce the highest performance results, confirming the
                        effectiveness of
                        Bidirectional Encoder Representations from Transformers (BERT) [] for Vietnamese. Hopefully,
                        PhoBert can
                        serve as a strong baseline for Vietnamese NLP research
                        and can be applied in many task with wonderful results.
                    </p>
                    <div class="content-img-wrapper">
                        <img src="phobert.png" class="content-img" />
                        <p class="content-img-caption">
                            <i> PhoBert, by Dat Quoc Nguyen, VinAI Research;Tuan Anh Nguyen, NVIDIA, USA.</i>
                        </p>
                    </div>
                </section>
                <section class="sub">
                    <h3 class="sub-title">Self-driving Car</h3>
                    <p class="content">
                        Self-driving cars are changing the way we live, work.
                        It make our life become more convenient, and create
                        safer and efficient roads. These revolutionary benefits
                        require massive computational power and effective algorithms beyond. Deep Learning has made
                        progress
                        in autonomous vehicles so far, with some important
                        prominent architectures such as Convolutional Neural
                        Networks (CNNs), Recurrent Neural Networks (RNNs),
                        etc. These Deep Learning algorithms enable self-driving
                        car possess some functions such as signs detection,
                        pedestrian detection in order to make decisions for operating. In general, those algorithms
                        will:
                    </p>
                    <div class="content">
                        <ul class="list">
                            <li class="list-item">
                                Process the images
                            </li>
                            <li class="list-item">
                                Output steering command based on images’s content.
                            </li>
                            <li class="list-item">
                                Output desired speed for current road condition.
                            </li>
                            <li class="list-item">
                                Feedback the actual speed of the car
                                (accelerate/deccelerate).
                            </li>
                        </ul>
                    </div>
                    <div class="content-img-wrapper">
                        <img src="drive.jpg" class="content-img" />
                        <p class="content-img-caption">
                            <i>Self-driving car Tesla with Autopilot.</i>
                        </p>
                    </div>
                    <h4>Levels of Autonomy</h4>
                    <div class="content">
                        <ul class="list">
                            <li class="list-item">
                                <strong>Level 0:</strong> All system are controlled by human.
                            </li>
                            <li class="list-item">
                                <strong>Level 1:</strong> Certain systems, such as cruise control
                                or automatic braking, maybe controlled by the car,
                                once at a time.
                            </li>
                            <li class="list-item">
                                <strong>Level 2:</strong> The car offers at least two simultaneous
                                automated functions, like accelearation and steering, but requires humans for safe
                                operations.
                            </li>
                            <li class="list-item">
                                <strong>Level 3:</strong> The car can manage all safety-critical
                                functions under certain conditions, but the driver
                                is expected to take over when alerted.
                            </li>
                            <li class="list-item">
                                <strong>Level 4:</strong> The car is fully-autonomous in some driving scenarios, though
                                not all.
                            </li>
                            <li class="list-item">
                                <strong>Level 5:</strong> The car is completely capable of selfdriving in every
                                situation.
                            </li>
                        </ul>
                    </div>
                    <p class="section">
                        Though still in its infancy, self-driving technology is
                        becoming increasingly common and could radically
                        transform our transportation system (and by extension,
                        our economy and society). Based on automaker and
                        technology company estimates, level 4 self-driving cars
                        could be for sale in the next several years.
                    </p>
                </section>
            </section>
            <section class="content">
                <h2>Conclusion</h2>
                <p class="content">
                    Today, Deep Learning is present in our lives in many
                    different ways that we may not observe. Google’s voice
                    recognition, Netflix and Amazon’s recommending systems; Amazon’s Alexa, Microsoft’s Cortana, or
                    Apple’s
                    Siri, etc. The greater the experience of Deep Learning
                    algorithms, the more effective thye become. With that
                    motivation and a vast amount of data surrounding us,
                    Deep Learning has a potential to become extraodinary
                    and accomplish great applications to human lives.
                </p>
            </section>
            <section class="content">
                <h2>Reference</h2>
                <div class="content">
                    <ul class="ref-list list">
                        <li class="list-item">
                            <div class="ref-item">
                                <span class="ref-num">[1]</span>
                                <span class="ref-content">Bengio, Y.; Courville, A.; Vincent, P. (2013). Representation
                                    Learning: A
                                    Review and New
                                    Perspectives. IEEE Transactions on Pattern Analysis and
                                    Machine Intelligence, 2013.
                                </span>
                            </div>
                        </li>
                        <li class="list-item">
                            <div class="ref-item">
                                <span class="ref-num">[2]</span>
                                <span class="ref-content">
                                    Mitchell, Tom Machine Learning. New York: McGraw Hill, 1997.
                                </span>
                            </div>
                        </li>
                        <li class="list-item">
                            <div class="ref-item">
                                <span class="ref-num">[3]</span>
                                <span class="ref-content">
                                    McCulloch, Warren; Walter Pitts A Logical Calculus of Ideas Immanent in Nervous
                                    Activity, 1943.
                                </span>
                            </div>
                        </li>
                        <li class="list-item">
                            <div class="ref-item">
                                <span class="ref-num">[4]</span>
                                <span class="ref-content">
                                    David A. Freedman. Statistical Models: Theory
                                    and Practice. Cambridge University Press A simple
                                    regression equation has on the right hand side an
                                    intercept and an explanatory variable with a slope
                                    coefficient. A multiple regression e right hand
                                </span>
                            </div>
                        </li>
                        <li class="list-item">
                            <div class="ref-item">
                                <span class="ref-num">[5]</span>
                                <span class="ref-content">
                                    Ho, Tin Kam. Random Decision Forests, 1995.
                                </span>
                            </div>
                        </li>
                        <li class="list-item">
                            <div class="ref-item">
                                <span class="ref-num">[6]</span>
                                <span class="ref-content">
                                    Altman, Naomi S. An introduction to kernel and
                                    nearest-neighbor nonparametric regression, 1992.
                                </span>
                            </div>
                        </li>
                        <li class="list-item">
                            <div class="ref-item">
                                <span class="ref-num">[7-8]</span>
                                <span class="ref-content">
                                    Fukushima, Kunihiko. Neocognitron: A Selforganizing Neural Network Model for a
                                    Mechanism
                                    of Pattern Recognition Unaffected by Shift in Position, 1980.
                                </span>
                            </div>
                        </li>
                        <li class="list-item">
                            <div class="ref-item">
                                <span class="ref-num">[9]</span>
                                <span class="ref-content">
                                    Rumelhart, David E.; Hinton, Geoffrey E.;
                                    Williams, Ronald J. Learning representations by
                                    back-propagating errors, 1986.
                                </span>
                            </div>
                        </li>
                        <li class="list-item">
                            <div class="ref-item">
                                <span class="ref-num">[10]</span>
                                <span class="ref-content">
                                    Cortes, Corinna; Vapnik, Vladimir N. Supportvector networks, 1995.
                                </span>
                            </div>
                        </li>
                        <li class="list-item">
                            <div class="ref-item">
                                <span class="ref-num">[11-12]</span>
                                <span class="ref-content">
                                    Sepp Hochreiter; J¨urgen Schmidhuber. Long
                                    short-term memory, 1997.
                                </span>
                            </div>
                        </li>
                    </ul>
                </div>
            </section>
        </article>
        <div id="infomation">
            <div class="author">
                <h3 class="author-name">Lê Nguyễn Hoàng Nhân</h3>
                <p class="author-intro">
                    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec sit amet augue sit amet lacus mattis
                    congue id eget mauris. Mauris turpis enim, rutrum vel imperdiet a, efficitur ac lectus. Proin et
                    faucibus sapien. Nulla facilisi.
                </p>

                <h4 class="author-extra">Student ID</h4>
                <p class="author-extra-content">
                    2052625
                </p>

                <h4 class="author-extra">Education</h4>
                <p class="author-extra-content">
                    Ho Chi Minh University of Technology
                </p>

                <h4 class="author-extra">Location</h4>
                <p class="author-extra-content">
                    Ho Chi Minh City, Vietnam
                </p>
            </div>

            <div class="author">
                <h3 class="author-name">Nguyễn Hồ Quang</h3>
                <p class="author-intro">
                    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec sit amet augue sit amet lacus mattis
                    congue id eget mauris. Mauris turpis enim, rutrum vel imperdiet a, efficitur ac lectus. Proin et
                    faucibus sapien. Nulla facilisi.
                </p>

                <h4 class="author-extra">Student ID</h4>
                <p class="author-extra-content">
                    2052625
                </p>

                <h4 class="author-extra">Education</h4>
                <p class="author-extra-content">
                    Ho Chi Minh University of Technology
                </p>

                <h4 class="author-extra">Location</h4>
                <p class="author-extra-content">
                    Ho Chi Minh City, Vietnam
                </p>
            </div>
        </div>
    </div>

</body>

</html>